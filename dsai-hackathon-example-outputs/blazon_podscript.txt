on this computer. This meeting is being recorded. Oh my gosh, there we go. All right, well, first of all, thank you everyone for joining in. This is a pre-conversation that we're having prior to the DSAI ChatGPT hackathon that is occurring this Friday 12th of May, it's actually tomorrow, with a kickoff event and then it goes into Sunday where they'll be judging for the participants that are taking part in this, the solutions that they come up with. My name is Mark, I'm the co-president of DSAI and there's other members here as well. We've got Ray and Jennifer who are helping us out with this and we've got Jian here as well. Thank you for joining in and we've got some other guests. But we're here today, not so much to talk about the hackathon, we can do that a little bit, but we're here to talk about Blazon and I'd never heard of them before, but anytime you're developing something, you need to have really good tools because otherwise you're just building from scratch. So why don't we kick off with Ray, if you can tell us a little bit about like how did you find out about Blazon? What is it? Why did you think that this is something really interesting? Yeah, so I know Blazon because well first they reached out to me in the first place. For those that don't know, I regularly post AI content updates, opinions on LinkedIn, TikTok and YouTube. So I like to just post regularly on AI and then yeah, one of the members saw my content and decided to reach out to me and connect. And it was just started off as like an informal, some just a casual chat and then when this hackathon came up and we're looking for like sponsors and people to support the hackathon, they were one of the people that I followed immediately to potentially provide some of the tech stack and the tech options for participants at the hackathon. And yeah, Blazon, hopefully they're joining soon, but like yeah, they're basically a startup based in America and they provide two types of services. One is a service and one is an API. So they provide like this sort of this package, this library to help people build AI models quite effectively and quickly. And the other service is a no-code development platform where you can just drag and drop boxes to create automated workflows. So that's basically, I don't want to steal too much of their funding, I'll leave it to them to explain what they do. But yeah, that's the general gist of it. Yeah. Fantastic. And I think we're going to get a demo of them when they do come on so that people can see it. There are ways like, well, we might just pull up like the website so we can actually show people what that looks like and how you sign in and all that to the app and everything because it's pretty straightforward. These guys are a startup, right? Right? Yeah, they are. Yeah. And the stuff that they do, have you been using it yourself, the actual tools and stuff? Yeah, it's actually, one of the no-code platform is really helpful for just experimenting with automation ideas. Typically when you have like a startup idea or you want to just automate something, but you don't want to commit a lot of time yet to really build it out or you don't want to code it right now. You just want to just play out and see how it works. In theory, you can use that no-code platform, just drag and drop really quickly. Within like a minute, you can build an automated workflow and just test it out if the idea is valid. And then you can then consider going on to build the idea more properly or using their capabilities API, which Mark is showing on the screen right here. Yeah. So I've just got the Blazon thing on screen. Let me just minimize this part here that I can only see there. So yeah, there's some documentation here. You can see the website, what they focus on as Ray said, the web search. There's some natural language stuff to SQL. So just really helping coders who want to build, because I think it's become even more apparent now with chatGBT that, hey, someone that doesn't even know code, they can just get into chatGBT and actually start doing some stuff there. But when I went to this one and I went to the start building, you've got the Google sign in or you can do with GitHub. So if I just do Google and you're going to see here the 30,000 Google accounts that I have, because there's various businesses that are being run here. I'll do my main work one here. And Ray, do you want to just talk us through like what we're seeing on screen here? Yeah. So they have like a central console dashboard to manage their credits and just also just a way to access the documentation and also your API key, which Mark is kindly showing off for you guys here. But yeah. I'll just keep hitting regenerate so that it forgets. But anyway, let's go to, or I haven't used any of the credits and stuff yet. So that's all fine. Jesse. Jesse's here. Hey, Jesse. Hello, Mark. Welcome. Hey, Ray. Good to see you guys. Good to see you too. I was just sharing screen as we were just waiting for you guys, but let's go back to the start. Let's get everyone up here. So thanks for joining in. And whereabouts are you, Jesse? I knew you were West Coast, but I wasn't quite sure. San Francisco. Fantastic. I do miss that place and the hills that give you a really good workout wherever you're walking. So, mate, thank you so much for joining us. Ray and I were just starting off on, well, basically how we found out about Blaze and it's through Ray that's been, you know, looking into it and you guys have seen his content and yeah, just from that. But do you want to give us, give the crowd before we start sharing like your screen and stuff and you take us through the demo, do you want to just give us a bit of background about why you guys created Blazon? What's, what is it all about? It'd be great to hear from you guys. Yeah, absolutely. So at Blazon, we're building safe and trusted software components on top of large language models to help people like those in the audience build safer, faster, and more reliably. Fantastic. And how has it been going so far in terms of, you know, getting the build and continually adding new features to the tooling that you've got and, you know, getting feedback from customers? How has the startup kind of journey been for you guys so far? Quite well. The space is very exciting. And I think that we're building a lot of new stuff in terms of the amount of security that we're building. So we're still pretty early, but we've gotten some pretty positive feedback from early users and we're really looking forward to getting more. Fantastic to hear. And hopefully, you know, from the hackathon that we have over this weekend, we've got a lot of new stuff coming in. So I think it's going to be pretty exciting to hear more about what's going on in the community. I think it's going to be really cool to see hopefully, you know, from the hackathon that we have over this weekend, we can introduce some more new users from down under to what it is that you guys are doing there. Ray, did you want to, you know, kickstart with some other questions that you had? No, no further questions from me. I think it's good just to get started with the demo. I think everyone here is keen to see what Blazon can offer. But before we get into that, I think there's two kind of product streams that you've got, or at least from what I've heard about, because I'm very new to Blazon as well, but there's the capabilities tooling. And I think that's what we're going through now. And then there's also multi-flow and we'll go through that, I guess, on the Saturday. But just so that people understand and can get it in their mind, is it first of all, those two are the main tools, Blazon capabilities and then multi-flow. Are there other tools as well, or is it just those two? And then if it is those two, like how do that, like, what are they? What at a high level for people to understand in the room what they are? Yeah, those are the two main tools. So Blazon capabilities is supposed to be more developer friendly. It's a software package combined with an API with prepackaged components and capabilities that work out of the box that make it easy for people to build whatever applications they want and just integrate it directly into their code base with zero friction. Multi-flow is meant to be a low code workflow building tool. It's meant to be more friendly for people with maybe less of a technical or software engineering background. It comes with similar capabilities as come with Blazon capabilities, but the tooling offers less control and it's less friendly for people who need to really tightly integrate the tooling with their code base. Fantastic. All right. Well, I think that makes it clear, you know, and I can see how they are complimentary there. Did you have your screen you wanted to share? I'll make you co-host now so that you can do that. Is that commentary? Yeah, I think that works. Give me one moment. I have some pretty cool stuff to show you guys. Fantastic. We're always excited about that. And again, thank you for taking the time out of your evening to talk to some Aussies about Blazon. So really curious to see. So ready whenever you are. Ray, are you excited about this coming weekend and hearing more new things that get 어디 ladies about Ross and Huston and about Detroit? Yeah, definitely. Great. And so, how did you get involved? So a great question, still raw, as I said,
And we've got Jennifer here and Jian who I think are running off to Kmart to get a few things to help out the crowd. I'm pumped. I am pumped. Yeah, it's been a few, two months in the making organizing this. So yeah, it's been, it's finally cool to see it come together. And yeah, I'm sure it'll be a fantastic weekend for those who are. How are you feeling, Jennifer? Super excited. Can't wait to see what everyone comes up with. See the creativity and the tech work in action. Fantastic. All right. Well, look, we've got the screen up and running now, so I'll just unpin everyone and make you the main one and we'll go on mute and the show is yours, Jesse. All right. Thank you guys for your attention. So I'm going to walk you guys through the capabilities that we currently have available through the API. And then I'll walk you through some examples showing how you can build really magical looking workflows from the software components that we provide. So currently we focus on three main capabilities. The flagship one is what we call structured synthesis. So this solves the problem of when you need to parse some form of structured data from completely unstructured input. So for example, if you're reading a financial document and you need to convert it into a spreadsheet, right? Or maybe you're processing a bunch of incoming emails and you need to extract a list of action items along with dates for when those things are actionable or due. So that's a problem which is not solved out of the box by language models by default. That's sort of something that you have to add on top. And we provide a push button way for developers like you guys to go and just take this component, drop it on top of whatever sort of workflow you have and get a structured output. So how these structured inputs and outputs are specified are currently through Pydantic classes in the Python SDK. So Pydantic is a format specification and parsing library that's frequently used in API servers. And here it's convenient because you know exactly what the shape of the inputs and the outputs are. And once you supply these and you also give a natural language instruction for how the input should be transformed into the output, you can implement that capability as an instance of the structured synthesis capability, which you can invoke by either calling the capability through URI or by using the LLM decorator, which converts any Python function into a capability. So let me jump into a source code file and show you guys what I mean. So let's take this and try running the example. So for those of you following along at home, you'll want to pop open a terminal somewhere, you know, spin up a Python virtual environment if you have one and just type pip install capabilities. So I already have it installed, so I'm not going to reinstall it. But if you just type this, then you should be able to access and run all of the demos, which I'm about to show you guys. So here, normally we think of translation as sort of this sequence to sequence task, right? Like I say a sentence in English and you want a sentence in French or Chinese. But to demonstrate the power of the structured synthesis endpoint, we're going to try an elaboration of this where not only do we have a sentence level translation, but we have a list of word level translations, right? So like, what does it look like if I literally transliterate each word of some sentence, which I utter into the target language? So we can implement this in the capabilities library as follows. So you specify a word translation through this Pydantic data class. So there's a source word and a target word, right? And then you specify a translation output. So this is a translation of the input, and this is a list of word to word translations. All right. And so now all you have to do is you type a function like this. So you say def translate. There's some input text, which is a string. And all you do is provide a type annotation saying that, well, the output has to be a translation output. And so now all we have to do is we attach the LLM decorator, thereby turning this into an LLM function. And from the magic in the capabilities library, we can just run this function. And what will happen is that our structured synthesis endpoint will provide an implementation of this function, which uses a language model to perform the task, which is here in the doc string. So the language model will be conditioned on these instructions, and it will produce an output, which is a value of this data type. So we'll get a translation and a list of word translations. So if I were to run this. So the API is firing right now. So some other examples of things that we can do with the LLM functions include, well, I mean, you can really provide an arbitrary pedantic data class as the output type. So you can do things like class. So we can create, say, a bunch of bullet pointed summaries of some piece of text. So I could say, well, a bullet point is some string. It's a list of strings. I was just going to say, Jesse, that coming from a financial markets background and having to pour through a whole heap of PDFs and other kind of source info just to find something, just seeing how there's something simple like this, even if you're not from a financial background, just having to do these processes takes so much time. So it's really interesting that you've got this tooling. Please continue. Oh, yeah. Yeah, I mean, we can do like a more financial example, if you like. In fact, like that might actually be interesting. So why don't we try like a financial metrics example. We're speaking with data? You're speaking? I'm sorry? I'm calling you from microphone. Hang on. Sorry, someone's mic is on. Let me just sort them out. Apologies. I'm receiving you on TV. So I'm just calling you. So while the customer support issue is being resolved. There we go. There we go. So we can also try extracting a table. So what's a table? So a table is a list of rows. And what's a row? So a row is just a list of cells. And each cell is a string. And now we can take in some piece of text and we can turn it into a table. So an example of instructions for this thing would be, let's see. So extract a list of all, let's see, subjects and verbs from the following passage. That's rows of table. Each row should have two cells, the first cell should be a subject and the second cell should be a verb. So for example, the passage is, I made an AI assistant. So the first row should be a subject.
I am made. Okay. So these all constitute the ingredients for an LLM function call. So now we can just write the actual function signature. Right, so we're going to pass in a passage and then extract a table. So this becomes the doc string. And now all we have to do is attach an LLM decorator. So now that we have this, we can just run it on some passage. So, Snapple, let's try running it on this. And in the meantime, let's comment this out. All right, so is everyone following here? So I'm demonstrating another example of an LLM function where we're trying to do some sort of structured parsing of an English sentence. So let's try running this again. I have no idea if this is... Gotta love live demos, right? I'm sorry? I said, you gotta love live demos. You gotta pray to the demo gods that, you know, things work and all that. Yeah, I've never done this before. So, you know, it's always a roll of the dice. But you know, the technology works, as you can see here. So you can see here that we've got a list of rows and these are all subjects and verbs. Right, and the interesting part is that the output is a full Python object. So it's precisely an instance of this table schema, which means that you can take the rows and then you can manipulate them later, right? So another example of... So another example... Possibly involving some financial metrics might involve... So let me pull up an example. Yeah, I mean, if it's... Yeah, I'll go ahead. Have you got something there in the financial space? Yeah, so I'm... So I just have a couple of paragraphs from, I think, NVIDIA's Q4 2022 earnings report. Q4 2022 earnings call. So we can try extracting some metrics from that. So let's try like an earnings call snippet. All right. And then this one doesn't contain too much interesting information. It's a pretty long input. So, all right. So I believe this is Colette Cress talking during the earnings call, talking about Q3 revenue. So now we can change this up. So you can change this doc string from extract a list to extract a table of all financial metrics mentioned in the input passage. So each row should have the following format. So metric name, metric value, supporting passage. Okay, so should have three cells which have the following content. So what I'm asking it to do is come up with a name for the metric. So for example, this one should be revenue. Then extract the numeric value. And then I'm gonna go ahead and extract the metric. So I'm gonna go ahead and extract the metric. So I'm gonna extract the numeric value. And then extract, so the metric unit. So for example, the passage is, let's say net revenue retention, so Jay Ponna. I am running example on unstructured data right now. So this is just a raw string. So for example, if the passage is net revenue retention, was only $500 last quarter, right? Then the row should look like an R 500 USD. Or 500 USD. So now that we've given that sort of instruction, now we can just try running this function and seeing what happens. But as you can see, it's very easy, right? It used to be that like, you had to write some like terrible regex, right? Or like for stuff like this, you couldn't even write a regex at all. But now it's as easy as- You had regex experts in the teams that just, for some reason or other, they just were the ones that knew regex, but to everyone else, like even some of the coders is just like, what is this? So now it's very interesting, sorry. Yeah, absolutely. Yeah, that's not to say that like, all the regex experts are gonna be like out of a job and sent to the coal mines. It just means that people are gonna spend a lot more time writing code like this instead, right? It's not like normal coding, but instead you're writing a bunch of instructions in natural language. And you're sort of just letting this AGI that sits behind an API do a bunch of the work for you. Did you see the, whilst this is running, did you see the thing that came out over the weekend? I think it was, it's part of the new plugins. I think it was the code interpreter or something where people like, once you get access to the plugins, you've got the code interpreter and you can type within chatgbt, here's some data, create this kind of chart for it. Like literally, my background is analytics, right? Data science analytics and having to do R and Python and having to figure out how to do all that kind of stuff. And now people can just type it literally. It's confronting, but it's necessary because there's a lot of wasted time doing those things when we can actually be doing stuff with the insights and taking actions, right? Yeah, I think it represents this shift in how knowledge work is going to be done, right? Like a lot more of it is just going to be powered by natural language, kicking off previously unforeseen amounts of automation. Right, like there's going to be a day when people just write software that looks just like this, right? It's hundreds and hundreds of source code files that look just like this. And each of these function calls kicks off, you know, like a code interpreter using agent, you know, like the one that you were just talking about. Right, and there's like a very like simple version of this that's running in the capabilities back end right now. As you can see here, it correctly parsed. So it correctly parsed a bunch of metrics, right? And it went and extracted some supporting passages as well about like how there's very solid performance, you know, in the face of macroeconomic challenges. So hopefully you guys might be able to just plug and play with the capabilities API and start supercharging your own work. So the reason why you guys should use this instead of, you know, one of the other hundred random libraries out there is that this just works out the box, right? I took this, you can just pip install it. You just write something like this, right? And then before you know it, you just do something like for file and like very long list of files, right? Extract metrics on the file, right? And that's like a whole day's work compressed into five minutes. And that works out of the box. That is ridiculous. Yeah, and it's something which is, so which is powered by GPT currently in the backend, but we do multiplex over multiple model providers. So our perspective on this is that you guys shouldn't worry about the API calls. You guys shouldn't worry about the language before it passes and all the technical optimizations. Instead, what you guys should experience is just the magic of doing something like this. Yeah, I think you're referring to Ray's first question there. Like what's the advantage? So Ray had to go, I think he's, I mean, he's got to go do lawyering kind of stuff. Cause apart from being a awesome AI coder, break dancer, YouTube personality, yeah, he has to do a day job as a lawyer. But he said, first of all, there folks, if you didn't see it, what's the advantage? Or, you know, cause this is recorded. So for people listening in, the question was, what's the advantage?
what's the advantage of using Blaze and capabilities over other APIs? And we'll ask the other questions that he's got after that, but you were going through the API stuff, Jesse. Yeah. So if you guys want to try this, just head over to, I'll type it here, docs.blazon.ai./.structured. And you'll see the usage example, which spawned this example that you see here on the screen. And if you just, so if you click around, you should be able to see a signup link. If you go to the homepage. Yeah, I signed up before and I gave away my key, but I regenerated it so that you can't steal my key, folks. And for everyone who's here right now, if you sign up, so we have a special offer for very early users of the product. So it'll be completely free for the time being. Users normally get like 100,000 API calls max per month. And we expect that that's more than you'll need, unless if you're doing some like super duper heavy duty stuff, in which case you should get in touch with our sales team. But that should hopefully be enough for you guys to just build stuff like this and, you know, get started. And hopefully you'll only be working like five minutes a day from now on. You can email us at support at blazon.ai or just email me directly, actually. So just jesse at blazon.ai. And supported lang, because I think you've covered this. So Ray was asking before he had to go, the underlying stack is a GPT powered. So you answered that and how to access capabilities. And you've got that on screen here. But he also asked about supported languages. I.e. is it only Python? Currently, our only SDK is in Python, but you can access the API through just post requests. And so it's pretty easy to set up something where you can just hit it with like an Axios request inside of TypeScript or through a curl command if you want. But currently, most of our tooling is in Python with support for TypeScript coming very soon. Good enough to start. Most people should be out. Sorry, most people should be able to handle that. He also asked if people are building using Blazon capabilities, would you be able to talk about how the licensing works? For example, is it under an iOS license or a specific license? And he also goes into what is the, like I know that there's this free and we appreciate free, the open source community loves that kind of stuff. And we're certainly keen to drum up support for what you're doing. But what is the subscription slash payment structure? Is that something that you can talk about, Jesse? Yeah, so capabilities is completely open source. And there, so there's a GitHub repository where you can see how a lot of the magic works behind the scenes. And then the structured synthesis and the two other core capabilities that we have on the documentation are powered by an API backend. And that's something which is closed source, but you guys can swap in your own backends if you want. We just provide ours for convenience and they work out of the box. And currently the product is completely free. There's a usage cap of 100,000 API requests per month. We just really wanna get feedback from builders and make sure that you guys can build as quickly and reliably as possible. Fantastic. Is there anything else in the demo here? I don't see other questions. I think someone asked about how to get in touch. And so you've given that detail. I can see Pranav's here as well. So thank you for joining. Not sure if you're on camera, but I'll ping you up there. But yeah, do you have any other things that you wanted to kind of show us here? Or is that the- Yeah, how am I doing on time? I'm pretty good. We got about 10 more minutes left. So why don't I walk you guys through a slightly more complex demo that sort of shows like the kind of magic that we can achieve if we chain a couple things together. So I've only shown you one piece of the capabilities library, which as you can see is pretty powerful already. These natural language instructions combined with like the sort of, no annotation power dev experience can be extremely flexible. But now let's do something even more interesting. So why don't we build a research tool? Right, so let's just spin this up. So what's the research tool gonna do? So you wanna learn about some topic. And so step one, the user enters a topic which they want to learn about. Okay, so step two, we wanna use the, use capabilities to help brainstorm some auxiliary topics or questions. Step three, we're going to gather, so we're gonna gather the information from relevant Wikipedia pages. And so we're gonna use the, the information from relevant Wikipedia pages or the auxiliary topics. Okay, and then step four, we're gonna download all the pages. And we're gonna spin up a Q&A system that uses a vector database. All right, so what this does is, this kind of lets you become an expert, hopefully very quickly about some topic of your choosing. So first, so first we have to build the thing that goes from one to two, right? So let's write something which can help us brainstorm topics. I wonder, I wonder if this works. I'm up with it, just pockets, which are related to, would be helpful. So let's give this a whirl. Whilst you're doing that and running it, do you guys get into the whole like prompt engineering and learning what the chat GPT and GPT models anyway, like what are the better kind of ways to ask questions? That's always been a hot, interesting topic for our audience. I'm just curious if you guys play around and experiment with that kind of stuff there into capabilities that you've got here. Oh yeah, part of the value add of capabilities is that we take away the pain of prompt engineering. So you don't, so it used to be that people would have to learn how to talk to each model, right? Like, oh, this model like responds really well when you talk to it in Chinese and then you ask it to like solve a coding problem or something like that, because it was trained on like 500 billion tokens of Chinese source code files, right? It only knows Chinese doc strings. But with capabilities, we kind of abstract all of that away from you. So all you have to do is you just have to write a detailed instruction like this, right? So you refer to whatever the input was and then you just give a detailed specification for what you want to happen. So given the input, how to transform it into the output. And then that just works. And when that's fired against our capabilities back end, we find a way to multiplex it over multiple models so that from your point of view, the same thing happens, right? Like this magic happens when you run the function. But from our point of view, we take care of all the messy prompting. So you only have to learn one style of prompting here. So if we fire off this research tool, you can see that it correctly comes up with a list of strings, which are all related topics that would be related to and helpful for learning about the input topic, which is nuclear fusion. Okay, so this is great. So why don't we try conditioning this on a chain of thought actually? So maybe step zero would be to learn or it would be to come up with a devious master plan to learn about the topic. Sounds like chaos GPT. Yeah, that's right. Yeah, and then there's step five, launch the nukes. Actually, maybe we should take that out of the video. We don't want the... It's called clean nuclear, it's fine.
Yeah, that's a great idea. Invent clean nuclear energy. There you go. Yeah, we should hire you, Mark. All right, so let's come up with Devious Master Plan. So let's see. Plan. So we want to plan a master plan for learning about a topic. OK. And let's also condition this on the plan. So given the plan, we'll learn about the input topic. So now let's just try running these in sequence. The plan is equal to, let's call it P. All right, let's see if this works. Need to have some hold waiting music playing in the background. That's not a very devious plan. It looks like it's just talking about nuclear fusion. Come up with a plan. So we'll just do a plan. It's talking about nuclear fusion. Let's say curriculum. It touches on an interesting topic that very early on, like basic users, regardless of whether you know code or not, people will roll the dice with, say, chat GPT, because that's their initial kind of entrance to GPT models. They won't get an answer that they, they'll get an answer that they don't think is good enough and then just completely trash the thing and go, this doesn't work. It's about re-prompting, just like what you're doing right now. Yeah, that's right. Yeah, and in fact, versioning and caching and being able to backtrack and see all the previous versions of a prompt which you use is another pain point that we found a lot of users have when they're trying to build AI applications. And that's something which is actually coming very soon to capabilities. So inside of supported IDEs like VS Code, so we're going to have an extension which will just let you click on the annotation, which is above any of these functions, and just scroll through previous versions of the prompt so you can find the one that works best. So I bet if we try this now, so because we told it to come up with a curriculum instead, then we should be able to get something which should be more helpful for brainstorming the topics. But anyways, while that's running, let's go. I was just going to say on that being able to have version, it's not just useful for personal or individual usage, but for businesses as well. If you've got teams of people doing this kind of stuff, whether you're in marketing, HR operations, like it is good to have that ability to have versioning. So I can see business usage for this too. Sorry, what's happened? So I think with this case, it's, I think this is like a very simple example. So I think this is like a side effect of the instruction fine tuning, which chat GPT goes through. It seems to overfit to saying like a certain something about nuclear fusion, but that's OK. So let's scrap the plan. Let's just stick with the brainstorm topics. I think people are learning about capabilities as well as GPT limitations at the same time. So now let's implement two implies three. So assuming that we've come up with a list of auxiliary topics or questions, now let's gather information from relevant Wikipedia pages. So for the list of topics and questions. So the way that we're going to do this is I'm going to pull up a handy piece of code, which wraps a Wikipedia search function. So let's just import this into the file. And so what this does is it returns a list of dictionaries, which all have this form. So it's a title, a summary, and a URL. So we're just going to use each of the topics, which came over here as an input for the search function. So search results. It'll be the application of Wikipedia search to the topic for each topic and topics. Actually, let's put this into a for loop. So let's take this for a spin. So we're going to brainstorm the topics. Now we're going to run these through Wikipedia search. And then while we're at it, so let's just put the finishing touch. So let's take a look at this function. So it returns the top five results. And I think we should just grab the top three. So search results. So iterate through search results. And we're just going to add the first three items inside of x. So this will be a concatenation of the top search results. So now we want to grab the URLs. And once we have the URLs, we're going to turn these into a vector database and then start doing question answering over them. So to do that, we're going to use the search abstractions, which we have inside of capabilities. So if you go to the Blazon docs, you'll see that we also have a section of the docs where we talk about search. And we have an entire library, which is dedicated to tooling on top of vector search. So vector search is helpful when you have a large collection of documents, like for example, a subset of Wikipedia that you need to do search over. And how state of the art question answering systems with language models work right now is you have some question that you want to ask about some large corpus, which is embedded inside of the vector search. So you submit a query to the vector database, and then you process the outputs to produce an answer to your question. And that's something which meshes very well with the document Q&A endpoint, which will take in a document that's created from the search results and create an answer for you that has citations. So the remaining steps are create the search index, and then run document question answering. So how do we create the search index? Well, going to look at the search example. So I was doing another version of this with the Notion documentation. There are only about 179 pages on the Notion documentation page. So if anyone wants to try creating their own Q&A over all the Notion documentation, just shoot me or Pranav an email afterwards. And I can send this file to you. Very good idea. So just going to import this stuff, which will be helpful for creating the search index. So these are the links. So now we just have to create a search index where you pass it an embedding model. So the search index is the heart of a vector database, so in our tower of abstractions. And it's powered by some kind of embedding model, which can produce vector representations of documents. So in this case, we actually make it very easy. It's as easy as.
like importing search index from capabilities.search. And now all we have to do is we just populate the index. So that involves this. So this populates the index and gets a ready question answer. So once the index is created, now we can just use a very simple while loop. And with that in place, this should be ready to go. So what's happening inside of here? What's happening inside of here is that we've, so we've gathered this list of URLs, right? So this list of URLs was gotten by a bunch of Wikipedia searches, which were sort of dreamt up and brainstormed with us by the language model. And once we've gathered that data from the internet, we pull it back to our machine and we create a search index. And now we can do a question answering loop, right? So what's happening inside of here is we grab some query from the user, we pose the query against the search index. And what's happening here is we do a lot of magic behind the scenes to hide all the machinery at the vector database. So from your point of view, all that you have to do is pose a query and you get back a list of results. So we show all the results for fun, and then we take those results and process them using Blazon's document QA capability, which you can invoke by just calling capability with the URI Blazon slash document QA. And then that processes it into a final answer. So let's try this. In fact, let's do this with only the first result for the sake of speed. And there are probably gonna be a couple of bugs, but we'll be able to iron them out, I'm sure. Like that one, that's interesting. Where's that coming from? Let's see. Maybe I should remove this. Great. Okay, so as you can see, it ran very quickly. So we immediately got a list of topics. So to learn about nuclear fusion, we should learn about plasma physics and nuclear physics and energy and thermonuclear reactions and fusion reactors. And so these are all the search results, which we just pulled from Wikipedia. And I need to define the search index. Oops. All right. And so if we just run this one more time, we should get the exact same results. Yep. And then we can see here we're pulling all the results that we need from Wikipedia, and we pull up a bunch of related topics like magnetic confinement fusion and inertial confinement fusion. And so now we're creating the search index, which you can see by the, so the loading indicator at the bottom of the console. So what's happening now is that we're taking these documents and we're sending them through an embedding API to get vector embeddings for populating a database. Once that's complete, we'll be able to pose questions against that database. And we'll have our own private question answering bot that can answer questions over our own special subset of Wikipedia that is stored as a private search engine on our computer. So you can imagine applying something like this to, you know, like maybe you were like working in a mergers and acquisitions deal, right? And you need to ask some questions over like, like, you know, what's going on inside of the data room. Or maybe you're like trying to figure out like what's going on inside of some company's financials and they have like a bunch of publicly filed, you know, reports with the SEC. See, we're hitting a, okay, let's try restricting this. If we restrict this to, actually, I think I can get around this by deduplicating the links. It's a bit of a hack. There's a lot of use cases for something like this, whether it's, you know, finance that we're talking about now but lawyers and, you know, going through all heaps of legal documents just to find stuff that is typically inefficient, takes time to a lot of manual stuff, but trained the right way. They could use this too, as well as many other industries. There's a lot, ton of business use cases and I'm sure a lot of personal ones too. Oh yeah. Yeah, I'd be curious to hear from the audience, you know, what sorts of things like you guys would be interested in building, you know, especially if you're planning on participating in the hackathon later this week. Yeah, we had some, so we set some seed ideas for different industries and kind of use cases there. We've got a sponsor that wants to put in an idea. So we're waiting on that one to come through, but people are able to bring their own ideas to this, like there's, as long as it fits in the requirements for the hackathon of, you know, participating in a team, filling out the judge, you know, conforming to the judging criteria, because we want this to have some sort of business use case, can have the really cool code, but if it's not really tied to anything to do with an existing industry that you're solving a problem for, it's going to be harder. But yeah, people, if you've got questions, I know there's a few left here, people probably had to go, Ray had to go earlier, but feel free to ask questions, to speak up or type it into the chat. Whilst that's happening, we can see that there's a timer countdown at the bottom that you've got there. What is it? Yeah, that's right. It's processing this into a ready to go vector database. Here's one we prepared earlier. No. No, that's fine. Where are you guys seeing usage of this? Like what industries have you, have you been speaking to businesses? Is it mostly developers that you're talking to? Or yeah, where are you kind of seeing use cases for this yourself, Jesse and Pranav? Mostly developers who are interested in building question answering systems, building systems that can process large volumes of documents. People who are trying to navigate the current landscape of what sorts of tooling there are around large language model technology. Fantastic. And there's a lot we're seeing here, whether in business or even government. A lot of people before these large language models, they were using the, they're not rudimentary, but they're far more limited. Virtual assistant and chat bot type things, ones where they sit on top of a database and they basically, if you go to a website for an energy company, for example, that will give you options to click on and it will, once you click on something, there's not much that you're typing because it doesn't really know that. It tries to give you more of a guided kind of experience rather than this real natural language where people can actually speak about and explain what the problem is that they're having. And so we're seeing companies like that starting to look at how could they integrate with GPT models to make it a little bit easier for not just the customer, but also developers. And another use case that could be interesting here, given what you've been showing here around education, I don't know if you saw the TED talk recently from Sal Khan, the founder of Khan Academy. And he talks about GPT models for teachers and students, i.e. being a personal tutor for students. You might not be able to afford that normally, but now you can. You can have access to something there and also being a personal teaching assistant for teachers that they wouldn't normally have. But are you seeing use cases in the education space, whether college, high school, et cetera? Oh, there's absolutely a great deal of interest in applying language models to education. And in fact, demos like this one show how easy it is to spin up something which could be very helpful in education. Like a college student could just, sign up for capabilities, download the Python.
package, right? See 150 lines of code and they have a search engine that's ready to go. You know, with language models that can process an arbitrary subset of Wikipedia. I mean, you could imagine something like this being applied, like, not to Wikipedia per se, but say over, like, your own collection of course notes, right? That's something that could help people study for their finals. Like really the field's wide open. It's so easy to get straight to the bleeding edge. Absolutely. And what's interesting as well is that you see with things like, say, GPT-0, where it's supposed to be helping find where there is potential, this is mostly written by an AI type of plagiarism. However, when you run that GPT-0 model, which folks don't know, is based on the fact that humans talk, real humans talk with more perplexity and burstiness. And those are two types of ways, like how complex are your words and how long or short and different, you know, volatilities there are in the sentences that you have. But humans have a lot more dynamism. It's a lot more dynamic when they do that versus the original kind of GPT models. And so these things were there to detect whether or not something is AI. I don't know if you've played around with that before, Jesse, Pranav, have you guys seen GPT-0? I haven't played with GPT-0. It's interesting though, because it's supposed to detect and college professors were using it in the States to detect if someone was using chat GPT. But the problem is I would put in work from 10 years ago, stuff that I'd written, and it would say part of it was written by AI. So I think some of these detector tools are a bit limited as well. But yeah, I was curious if you guys had seen tools like that being used. We definitely plan on building out tooling like this inside of capabilities to ensure that the outputs of these sorts of LM functions are actually aligned with their specs. So currently we're sort of just praying, right, that the API correctly interprets the instructions here. And in fact, for many of the cases that we've seen, it does. Like for this one, it's sort of just came up with a list of topics about nuclear fusion off of that. But as for things like verifying how correct outputs are, making sure that certain outputs can be attributed to humans, or certain outputs can be attributed to parts of documents, that's absolutely something that we want to support. And it's something that we've begun to support in a rudimentary form with our document question answering endpoint, which we can give a whirl on right now, because the search index has finally finished computing. So what's a question that we can ask about fusion? What are the most promising finds of fusion reactors? So we can see here that it returned almost instantaneously with a list of chunks from Wikipedia articles. And you can see that the top results are actually pretty relevant to the question. Like for example, the first result talks about possible ways to design fusion reactors and what the leading designs are. So the current leading designs are the tokamak and inertial confinement by laser. And the others all talk about various sorts of fusion reactors as well. And you can see down here that the document question answering capability, which just works out of the box. You can just take a look at the docs, it's a few hundred words, and then just import it in your code. It returns a bullet pointed answer to the question, which is backed up by supporting passages. So- That's the hardest thing. The supporting passages is one of the biggest things that people have found just missing by using out of the box models, or even the fact that there are these hallucinations they're like, well, do I, you know, we try to tell people don't just copy and paste the output. You have to like read through it, but obviously that takes time as well. But if you've got something like this, where there is that support, this is pretty massive for research. Well done. Yeah. And here you can see that, you know, the supporting passages do support the bullet points, each of the bullet points do answer the question. So, you know, you could imagine just downloading capabilities, spinning up four of these at once, you know, like they tile your screen, it's like this God mode view, and they're just going out and crunching through petabytes of data on your behalf. So, you know, that sort of power is within your reach, you know, it's at your fingertips, just, you know, just go pip install capabilities. And what better way to, I think we have to wrap up now, but as I said, folks, this is a great webinar, folks, this is recorded. We'll share this with you guys, Jesse and Pranav as well, but we'll get this out on our YouTube pretty soon after it all downloads. So yeah, folks, there's emails there that Jesse, that you've shared, people want to learn more. Pranav, thank you very much. So docs.blazon.ai, B-L-A-Z-O-N.ai. Are you guys on like socials, LinkedIn, Twitter, people want to get in touch with you guys there, or is it preferable email only? Email would be best for now, but very soon we'll have a much larger presence. Okay, fantastic. I love what you guys are building. This is really cool. We saw great two different demos there, one on looking up finance kind of documentation, and then this other one on research. I'm sure there's going to be plenty of replays on this, but for now, Jesse and Pranav, thank you so, so much for taking the time out of... Is Pranav in the same city as you? You guys both in San Francisco? Yeah, we're based in San Francisco. Fantastic. Well, we've learned a lot today. I think we'll leave it there, but thank you so, so much. And yeah, folks, replay will be out soon. I'll just hit stop recording for now.